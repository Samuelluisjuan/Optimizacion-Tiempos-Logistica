# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vKpDrJkSU47Zy6vsrcq4JEU4SLHB2bnY
"""

import pandas as pd
import kagglehub
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
import scipy.stats as stats

"""# Breve descripción del conjunto de datos y resumen de sus atributos:
Delhivery es una destacada empresa de servicios logísticos y de cadena de suministro en India, conocida por su amplio alcance y sus eficientes soluciones de entrega, que aprovechan tecnología avanzada para garantizar entregas puntuales y fiables en diversas regiones. El análisis de este conjunto de datos ofrece información valiosa sobre las operaciones logísticas de Delhivery, revelando detalles sobre la eficiencia de los viajes, la optimización de rutas, los tipos de transporte y el rendimiento de las entregas. Proporciona una comprensión integral de la programación y ejecución de viajes, los factores que afectan a los tiempos de entrega y la optimización de rutas mediante motores de enrutamiento de código abierto. Este extenso conjunto de datos es crucial para mejorar las estrategias logísticas, optimizar el rendimiento de las entregas y tomar decisiones informadas en la gestión de la cadena de suministro. La empresa busca asistencia en la limpieza, manipulación y análisis de datos para extraer características útiles, respaldar modelos de pronóstico precisos y procesar datos de sus procesos de ingeniería.
"""

# Cargar el dataset
df = pd.read_csv("/content/delhivery.csv")

# Mostrar las primeras filas
df.head()

df.describe()

df.info()

# Mostrar resumen de valores faltantes
missing_values = df.isnull().sum()
print('Missing Values Summary:')
print(missing_values)

"""## Análisis de Valores Faltantes
Se realizó un análisis de valores faltantes en el dataset, identificando que la mayoría de las columnas cuentan con datos completos. Sin embargo, se detectaron valores nulos en dos variables específicas: source_name (293 registros) y destination_name (261 registros). Estas columnas representan los nombres de los centros de origen y destino, respectivamente.

A pesar de la ausencia de estos valores, las columnas source_center y destination_center no presentan datos faltantes, lo que sugiere que la información sobre los centros de operación sigue disponible en el dataset. Esto permite mitigar el impacto de los valores nulos, ya que se podrían utilizar identificadores de centro en lugar de nombres.

Dado que el resto de las variables no presentan valores faltantes, la integridad general del dataset se mantiene, permitiendo continuar con los análisis sin necesidad de imputación de datos en otras áreas.
"""

df = df.dropna()

# Separar los nombres de las columnas por tipo de datos
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_columns = df.select_dtypes(include=['object', 'bool']).columns.tolist()

print("Numeric columns:", numeric_columns)
print("\nCategorical columns:", categorical_columns)
df.shape

"""## Resumen de Tipos de Datos
El dataset de Delhivery contiene 144,316 registros y 24 columnas, distribuidas en:

  - Variables numéricas (11 columnas): incluyen tiempos de viaje, distancias y factores de corte.
  - Variables categóricas (13 columnas): comprenden identificadores de rutas, centros de operación y marcas de tiempo.

Esta clasificación facilita el análisis estadístico y la selección de técnicas adecuadas para cada tipo de variable.

# Plan inicial para la exploración de datos
Para el plan inicial de exploración de datos, crearemos histogramas, diagramas de caja, un diagrama de dispersión que compare osrm_time y actual_time, gráficos de estimación de densidad de kernel (KDE) y una matriz de correlación para todas las características numéricas.
"""

# Crear una figura con múltiples subgráficos (uno por cada variable numérica)
fig, axes = plt.subplots(11, 1, figsize=(12, 30))

#  Iterar sobre las columnas numéricas y generar histogramas
for i, c in enumerate(numeric_columns):
    df[c].hist(ax=axes[i], bins=30)
    axes[i].set_title(f'Histogram of {c}')
    axes[i].set_xlabel(c)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(11, 1, figsize=(12, 30))

for i, c in enumerate(numeric_columns):
    df.boxplot(column=c, ax=axes[i], vert=False)
    axes[i].set_title(f'Boxplot of {c}')
    axes[i].set_xlabel(c)

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(11, 1, figsize=(12, 30))

for i, c in enumerate(numeric_columns):
    df.boxplot(column=c, ax=axes[i], vert=False)
    axes[i].set_title(f'Boxplot of {c}')
    axes[i].set_xlabel(c)

plt.tight_layout()
plt.show()

"""## Observaciones principales:
  - La mayoría de las variables presentan una distribución sesgada a la derecha (cola larga a la derecha), lo que indica que la mayoría de los valores son bajos, pero hay algunos valores atípicos altos.
` - Variables como start_scan_to_end_scan, actual_time, osrm_time y osrm_distance tienen una gran concentración de valores cercanos a cero, lo que sugiere que la mayoría de los viajes o segmentos son cortos.
  - Factor y segment_factor muestran una distribución más extrema con valores atípicos significativos.

En general, estos histogramas indican que el dataset tiene una fuerte concentración en valores bajos, pero con presencia de valores atípicos que pueden requerir un análisis más detallado.
"""

# Set up the plot
fig, axes = plt.subplots(4, 3, figsize=(20, 20))
axes = axes.flatten()

# Create KDE plots for each numeric column
for i, column in enumerate(numeric_columns):
    sns.kdeplot(data=df[column], ax=axes[i], fill=True)
    axes[i].set_title(f'KDE of {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Density')

# Remove any unused subplots
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Gráficos KDE de distribuciones de variables

- Muestra la distribución de diferentes variables mediante gráficos de densidad (KDE).
- La mayoría de las variables presentan distribuciones sesgadas a la derecha, lo que indica que hay valores atípicos o una gran concentración de datos en valores bajos.
- Se observa que algunas variables como factor, segment_factor y segment_osrm_distance tienen una alta densidad en valores cercanos a cero.
"""

# Seleccionar columnas numéricas y calcular la matriz de correlación
corr_matrix = df[numeric_columns].corr()

# Reordenar las columnas para poner 'actual_time' primero
columns_order = ['actual_time'] + [col for col in corr_matrix.columns if col != 'actual_time']
corr_matrix = corr_matrix.reindex(columns=columns_order, index=columns_order)

# Crear un heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)
plt.title('Correlation Matrix (Target: actual_time)')
plt.show()

"""## Matriz de correlación)

- Presenta una matriz de correlación entre diversas variables, con especial enfoque en actual_time como objetivo.
- Variables como cutoff_factor, actual_distance_to_destination, osrm_time y osrm_distance tienen una alta correlación con actual_time (cercana a 1).
- Otras variables como factor y segment_factor tienen una correlación baja o negativa con la mayoría de las variables principales.
- Destaca que segment_osrm_distance y segment_osrm_time están fuertemente correlacionados entre sí (~0.95).
"""

# Calcular la asimetría de las columnas numéricas
skewness = df[numeric_columns].skew().sort_values(ascending=False)

print("Skewness of numeric features:")
print(skewness)

# Visualizar skewness
plt.figure(figsize=(12, 6))
skewness.plot(kind='bar')
plt.title('Skewness of Numeric Features')
plt.xlabel('Features')
plt.ylabel('Skewness')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Manejar valores atípicos para todas las columnas numéricas
def remove_outliers(df, columns):
    df_clean = df.copy()
    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
    return df_clean

df_clean = remove_outliers(df, numeric_columns)

# Imprima la forma antes y después de eliminar los valores atípicos
print(f"Shape before removing outliers: {df.shape}")
print(f"Shape after removing outliers: {df_clean.shape}")

# Calcular la asimetría de las columnas numéricas en df_clean
skewness_clean = df_clean[numeric_columns].skew().sort_values(ascending=False)

print("Skewness of numeric features in df_clean:")
print(skewness_clean)

# Convertir 'trip_creation_time' and 'od_start_time' to datetime
df_clean['trip_creation_time'] = pd.to_datetime(df_clean['trip_creation_time'])
df_clean['od_start_time'] = pd.to_datetime(df_clean['od_start_time'])

# Extraer time-based características
df_clean['creation_hour'] = df_clean['trip_creation_time'].dt.hour
df_clean['creation_day'] = df_clean['trip_creation_time'].dt.day_name()
df_clean['creation_month'] = df_clean['trip_creation_time'].dt.month_name()

# Calcular la diferencia horaria entre la creación y el inicio del viaje
df_clean['time_to_start'] = (df_clean['od_start_time'] - df_clean['trip_creation_time']).dt.total_seconds() / 3600

# Crear características de proporción
df_clean['distance_time_ratio'] = df_clean['actual_distance_to_destination'] / df_clean['actual_time']
df_clean['osrm_distance_time_ratio'] = df_clean['osrm_distance'] / df_clean['osrm_time']

# Calcular la asimetría de las columnas numéricas en df_clean, incluidas las nuevas funciones transformadas en logaritmos
numeric_columns_updated = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()
skewness_clean_updated = df_clean[numeric_columns_updated].skew().sort_values(ascending=False)

print("Updated skewness of numeric features in df_clean:")
print(skewness_clean_updated)

# Visualizar la asimetría actualizada-
plt.figure(figsize=(14, 6))
skewness_clean_updated.plot(kind='bar')
plt.title('Updated Skewness of Numeric Features')
plt.xlabel('Features')
plt.ylabel('Skewness')
plt.xticks(rotation=90, ha='right')
plt.tight_layout()
plt.show()

# Características numéricas altamente sesgadas de la transformada logarítmica
skewed_features = ['start_scan_to_end_scan', 'time_to_start', 'actual_time',
                   'cutoff_factor', 'actual_distance_to_destination',
                   'osrm_distance', 'osrm_time']

for feature in skewed_features:
    df_clean[f'{feature}_log'] = np.log1p(df_clean[feature])

# Comprobar la asimetría después de la transformación
skewness_after = df_clean[[f'{feature}_log' for feature in skewed_features]].skew()
print("Skewness after log transformation:")
print(skewness_after)

df_clean.shape

# Iniciar LabelEncoder
label_encoder = LabelEncoder()
df_encoded = df_clean.copy()
categorical_columns = df_encoded.select_dtypes(include=['object', 'bool']).columns.tolist()
# Aplicar codificación de etiquetas a cada columna categórica
for col in categorical_columns:
    try:
        df_encoded[col] = label_encoder.fit_transform(df_encoded[col])
    except ValueError:
        print(f"Could not convert string to float in column {col}")

df_encoded.info()

# Seleccionar características en función de la correlación con 'actual_time'
correlation_threshold = 0.1
corr_with_target = df_encoded.corr()['actual_time'].abs()
selected_features = corr_with_target[corr_with_target > correlation_threshold].index.tolist()
selected_features.remove('actual_time')  # Remove target variable from features

# Crear el conjunto de características final
x = df_encoded[selected_features]
y = df_encoded['actual_time']
print(selected_features)

"""# Limpieza de Datos y Creación de Características
Se llevó a cabo un proceso de limpieza de datos y generación de nuevas características para mejorar la calidad del conjunto de datos y optimizar su uso en modelos de análisis.

En primer lugar, se aplicó el método del Rango Intercuartílico (IQR) para detectar y eliminar valores atípicos en todas las columnas numéricas. Como resultado, se obtuvo un conjunto de datos reducido y más representativo (df_clean).

Para abordar la asimetría en la distribución de los datos, se calcularon y visualizaron las métricas de sesgo antes y después de la eliminación de valores atípicos. Esto permitió evaluar el impacto de la limpieza en la distribución de las variables.

En cuanto a las características basadas en el tiempo, se convirtieron las columnas "trip_creation_time" y "od_start_time" al formato datetime. A partir de "trip_creation_time", se extrajeron nuevas variables como la hora, el día de la semana y el nombre del mes. También se calculó la diferencia de tiempo entre la creación del viaje y su inicio.

Se crearon características derivadas mediante la generación de relaciones entre variables existentes. Se definieron dos nuevas métricas:

distance_time_ratio: Relación entre la distancia real al destino y el tiempo real del viaje.
osrm_distance_time_ratio: Relación entre la distancia y el tiempo estimados por OSRM.
Como próximos pasos, se propone aplicar transformaciones (logarítmica, raíz cuadrada, etc.) para manejar la asimetría residual en los datos, codificar las variables categóricas y normalizar o estandarizar las características numéricas. Además, se realizará un análisis de correlación para seleccionar las características más relevantes.
"""

df_encoded.shape

fig, axes = plt.subplots(37, 1, figsize=(18, 60))

for i, c in enumerate(df_encoded.columns):
    df_encoded[c].hist(ax=axes[i], bins=30)
    axes[i].set_title(f'Histogram of {c}')
    axes[i].set_xlabel(c)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""# Formulación de Hipótesis
Se han planteado al menos tres hipótesis clave para analizar la eficiencia en los tiempos de entrega con base en los datos disponibles.

1. Hipótesis sobre la Eficiencia del Tiempo de Entrega
H1: Existe una correlación positiva significativa entre la distancia al destino y el tiempo real de entrega. A mayor distancia, se espera que el tiempo de entrega sea más prolongado debido al aumento en el tiempo de viaje y posibles retrasos.
2. Hipótesis sobre el Impacto del Tipo de Ruta
H2: El tipo de ruta (por ejemplo, exprés vs. estándar) influye significativamente en el tiempo real de entrega. Se espera que las rutas exprés tengan tiempos de entrega más cortos en comparación con las rutas estándar, debido a la priorización en el manejo y la reducción en el número de paradas.
3. Hipótesis sobre la Influencia del Momento del Día
H3: La hora en la que se crea un viaje (horas pico vs. horas valle) impacta significativamente en el tiempo real de entrega. Se espera que las entregas iniciadas en horas pico sean más lentas debido a la congestión del tráfico, en comparación con aquellas iniciadas en horas de menor demanda.
"""

# Hipótesis a probar: H1 - Existe una correlación positiva significativa entre la distancia al destino y el tiempo real de entrega.
# Hipótesis nula: No existe correlación entre la distancia al destino y el tiempo real de entrega.

# Extrayendo columnas relevantes
distance = df_encoded['actual_distance_to_destination']
delivery_time = df_encoded['actual_time']

# Cálculo del coeficiente de correlación de Pearson y el valor p

correlation, p_value = stats.pearsonr(distance, delivery_time)

print(f"Correlation Coefficient: {correlation}")
print(f"P-value: {p_value}")

# Interpretar Resultados
if p_value < 0.05:
    print("Rechace la hipótesis nula. Existe una correlación positiva estadísticamente significativa entre la distancia al destino y el tiempo real de entrega..")
else:
    print("No se rechaza la hipótesis nula. No existe una correlación estadísticamente significativa entre la distancia al destino y el tiempo real de entrega..")

"""# Próximos Pasos en el Análisis de Datos
Para optimizar el análisis y mejorar la precisión de los resultados, se recomienda seguir los siguientes pasos:

1. Preprocesamiento de Datos
Verificar valores faltantes y manejarlos de manera adecuada.
Identificar y tratar valores atípicos para evitar sesgos en el análisis.
Revisar y ajustar los tipos de datos según corresponda.
2. Ingeniería de Características
Crear nuevas variables que puedan aportar información relevante al análisis.
Generar relaciones entre características existentes para mejorar la interpretabilidad.
3. Análisis Exploratorio de Datos (EDA)
Visualizar la distribución de las variables clave.
Identificar patrones y relaciones entre las variables.
Analizar la influencia de diferentes factores en el tiempo de entrega.
4. Análisis Estadístico
Realizar pruebas estadísticas para validar las hipótesis planteadas.
Evaluar la significancia de las relaciones entre variables.
5. Construcción de Modelos Predictivos
Desarrollar modelos de aprendizaje automático para predecir los tiempos de entrega.
Probar diferentes algoritmos y técnicas para obtener el mejor rendimiento.
6. Evaluación de Modelos
Aplicar métricas adecuadas para medir la precisión de los modelos.
Ajustar hiperparámetros y realizar validaciones cruzadas para mejorar la robustez del modelo.
7. Despliegue del Modelo
Implementar el modelo más preciso para realizar predicciones en tiempo real.
8. Monitoreo Continuo
Supervisar el rendimiento del modelo en producción.
Realizar actualizaciones periódicas para mejorar su precisión y adaptabilidad.
Resumen del Conjunto de Datos
El dataset tiene una calidad general buena, con un tamaño de muestra amplio de más de 144,000 registros y una diversidad de características bien estructuradas. No se han identificado errores o inconsistencias evidentes.

Sin embargo, para mejorar aún más el análisis y la capacidad predictiva de los modelos, sería beneficioso contar con datos adicionales sobre factores externos que puedan influir en los tiempos de entrega, como:

Condiciones climáticas
Patrones de tráfico
Cierres de carreteras
Incluir esta información permitiría una mayor precisión en las predicciones y un entendimiento más completo de los factores que afectan los tiempos de entrega.
"""

